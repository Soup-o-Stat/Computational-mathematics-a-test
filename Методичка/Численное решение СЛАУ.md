# ЧИСЛЕННОЕ РЕШЕНИЕ СЛАУ

## Линейные векторные пространства, прямая и обратная задача, понятие нормы и обусловленности матриц

### Линейные векторные пространства

Линейное векторное пространство (или просто векторное пространство) — это множество объектов, называемых векторами, которое подчиняется следующим аксиомам

 - Коммутативность сложения: $u+v=v+u$
 - Ассоциативность сложения: $(u+v)+w=u+(v+w)$
 - Существование нейтрального элемента: Существует элемент 0, называемый нулевым вектором, такой что $u+0=u$
 - Существование противоположного элемента: Для каждого вектора u существует вектор -u, такой, что $u+(-u)=0$
 - Существование умножения на скаляр: Для каждого вектора u и числа a существует вектор au, и это умножение удовлетворяет таким свойствам:
 - - $a(u+v)=au+av$
 - - $(a+b)u=au+bu$
 - - $a(bu)=(ab)u$
 - - 1*u=u

 Векторное пространство может быть конечномерным или бесконечномерным, и элементы вектора могут быть функциями, числами или другими объектами

 ### Прямая и обратная задача

 - Прямая задача: Задана система уравнений (или модель), и необходимо найти решение, которое соответствует данным условиям. Пример: нахождение значений переменных при заданных уравнениях
 - Обратная задача: Заданы результаты (например, данные наблюдений), и нужно найти параметры системы, которые могли бы привести к этим результатам. Обратная задача обычно более сложна и часто связана с ошибками измерений или неопределенностью

 В контексте линейных систем, обратная задача может сводиться к нахождению неизвестных, таких как решения линейных уравнений при известных значениях

 ### Понятия и нормы обусловленности матриц

 - Норма вектора
 Норма вектора vv — это функция, которая отображает вектор в неотрицательное число и удовлетворяет следующим свойствам:
 - - Положительная определенность: $||v||>=0$, причем $||v||=0$ только для нулевого вектора
 - - Однородность: $||av||=|a|*||v||$ для любого скаляра a
 - - Неравенство треугольника: $||u+v||<=||u||+||v||$

 ### Нома матрицы

 Норма матрицы — это функция, которая отображает матрицу в неотрицательное число и удовлетворяет аналогичным свойствам, как и норма вектора

 ### Обусловленность матрицы

 Обусловленность матрицы — это мера чувствительности решения системы линейных уравнений $Ax=b$ к изменениям вектора b. Чем выше обусловленность матрицы, тем более чувствительна система к погрешностям в данных и вычислениям

 ## Прямые методы решения систем линейных алгебраических уравнений (СЛАУ). Метод Гаусса, Жордана-Гаусса

 ### Прямые методы решения систем линейных алгебраических уравнений (СЛАУ)
 Прямые методы решения СЛАУ включают последовательные вычисления, которые ведут к точному решению системы в конечное время (при условии отсутствия ошибок округления). Эти методы основываются на преобразовании исходной системы в такую форму, которую проще решить. Основные прямые методы — это метод Гаусса и метод Жордана-Гаусса.

 ### Метод Гаусса
 Метод Гаусса (или метод прямого исключения) — это алгоритм для решения системы линейных уравнений путём последовательного приведения матрицы системы к верхнетреугольному виду с помощью элементарных операций.
Этапы метода Гаусса:
 - Приведение матрицы к верхнетреугольному виду
 - - на каждом шаге выбираем ведущий элемент (обычно - элемент на главной диагонали) и преобразуем все элементы ниже этого в нули
 - - для этого используется перация элементарных преобразований: вычитание строки, умноженной на коэффициент, из других строк

 - Обратный ход
 - - После того, как система приведена к верхнетреугольному виду, решения для переменных можно найти методом обратного хода (или подстановки).
 - - Начинаем с последней строки, которая даёт значение последней переменной. Затем, зная значения переменных, находим решения для остальных переменных, двигаясь вверх по матрице.

 ### Этапы метода Жордана-Гаусса:
 - Приведение к верхнетреугольному виду
 - - Как и в методе Гаусса, приводим матрицу к верхнетреугольному виду с помощью элементарных операций.
 - Приведение к диагональному виду (метод Гаусса-Жордана):
 - - После того, как матрица привела к верхнетреугольному виду, метод Жордана дополнительно преобразует матрицу так, чтобы все элементы, не относящиеся к главной диагонали, стали равными нулям.
 - - Это достигается с помощью элементарных операций, аналогичных тем, которые использовались в методе Гаусса, но теперь применяются и к строкам выше диагонали.
 - Решение системы
 - - После приведения матрицы к диагональному виду переменные можно легко решить, так как каждая переменная будет стоять на своей диагонали, а остальные элементы в столбце будут равны нулю.

 ## Итерационные методы решения СЛАУ: метод простой и оптимальной простой итерации, метод Зейделя, метод релаксации

Итерационные методы используются для решения систем линейных алгебраических уравнений (СЛАУ) вида $Ax=b$, особенно если размерность системы велика. Эти методы основаны на последовательном приближении решения и обычно требуют предварительного приведения системы к специальной форме, обеспечивающей сходимость.

 ### Метод простой итерации

 Суть метода: Метод простой итерации (метод последовательных приближений) заключается в преобразовании исходной системы $Ax=b$ в эквивалентный вид: $x=Bx+c$, где $B=I-D^(-1)A$, $c=D^(-1)b$, D - диагональная часть матрицы A

 ### Алгоритм
 - Выбирается начальное положение $x^0$
 - Итерации проводятся по формуле $x^(k+1)=Bx^(k)+c$
 - Итерации продолжаются до тех пор, пока не выполнится условие сходимости $||x^(k+1)-x^k||<ε$, где ε - заданная точность
 
 ### Сходимость
 Метод сходится, если норма матрицы B удовлетворяет условию $||B||<1$. Это гарантирует уменьшение погрешности на каждом шаге

 ### Метод оптимальной простой итерации
 Метод оптимальной простой итерации (метод Чебышёва) представляет собой модификацию метода простой итерации, где применяется дополнительный параметр ττ для ускорения сходимости. Итерации вычисляются как: $x^(k+1)=x^k+τ(b-Ax^k)$. Оптимальный ττ выбирается так, чтобы минимизировать погрешность. Обычно ττ зависит от спектра собственных значений матрицы AA и может быть найден через анализ этого спектра.

 Метод быстрее сходится по сравнению с простой итерацией, особенно если матрица плохо обусловлена.

 ### Метод Зейделя
 Метод Зейделя (метод Гаусса-Зейделя) является модификацией метода простой итерации, где используются новые значения переменных сразу после их вычисления на текущем шаге. Система $Ax=b$ записывается в виде

 ![](https://github.com/Soup-o-Stat/Computational-mathematics-a-test/blob/main/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D0%B8%D1%87%D0%BA%D0%B0/%D0%A7%D0%A0%D0%A1_%D1%81%D0%BA%D1%80%D0%B8%D0%BD%D1%8B/screenshot1.PNG)

 Где $a_ii != 0$

### Алгоритм
 - Начальное приближение $x^0$ выбирается произвольно
 - Вычисляются новые значения $x_i^(k+1)$ (что за уебищные формулы??!?) последовательно для $i =1, 2, ..., n$
 - Процесс продолжается до выполнения условия сходимости

### Сходимость
Метод Зейделя сходится быстрее, чем простой итерационный метод, при выполнении одного из условий:
 - Матрица A является строго диагонально доминирующей:
 ![](https://github.com/Soup-o-Stat/Computational-mathematics-a-test/blob/main/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D0%B8%D1%87%D0%BA%D0%B0/%D0%A7%D0%A0%D0%A1_%D1%81%D0%BA%D1%80%D0%B8%D0%BD%D1%8B/screenshot2.PNG)

 - Матрица A является симметричной и положительно определенной

 ### Метод релаксации
 Метод релаксации (SOR — Successive Over-Relaxation) улучшает метод Зейделя, вводя дополнительный параметр релаксации ωω, который регулирует скорость сходимости.

 ### Итерационная формула

 Для системы $Ax=b$ используется следующая формула:
 ![](https://github.com/Soup-o-Stat/Computational-mathematics-a-test/blob/main/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4%D0%B8%D1%87%D0%BA%D0%B0/%D0%A7%D0%A0%D0%A1_%D1%81%D0%BA%D1%80%D0%B8%D0%BD%D1%8B/screenshot3.PNG)

 - Выбор w:
 - - w=1: метод Зейделя
 - - w>1: ускоряет сходимость (сверхрелаксация)
 - - w<1: замедляет сходимость (недорелаксация), применяется в случаях, когда метод Зейделя не сходится

 ### Сходимость
 Метод релаксации сходится, если матрица A является симметричной положительно определённой, а параметр ω находится в оптимальном диапазоне ($обычно 1<ω<2$).

 ## Ортогонализация матриц. Решение СЛАУ
 Ортогонализация матриц — это процесс приведения системы векторами (или строк/столбцов матрицы) к взаимно ортогональному виду, что облегчает решение систем линейных алгебраических уравнений (СЛАУ) и других задач линейной алгебры. Наиболее распространённый метод ортогонализации — процесс Грама-Шмидта. Также широко применяются методы разложения, такие как QR-разложение.
 Ортогонализация позволяет улучшить численную устойчивость решения, так как операции с ортогональными матрицами ($Q^⊤Q=I$) минимизируют накопление ошибок округления.
 Преобразование матрицы в ортогональную форму может снизить обусловленность задачи, что делает решение более точным.